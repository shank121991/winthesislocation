{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values does not match length of index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-2c46753e0dd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0mupdate_hourly_weights_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mcreate_save_seperate_trasition_matrices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m         \u001b[0mcreate_save_markov_chains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m         \u001b[0mprev_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_date\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-78-2c46753e0dd5>\u001b[0m in \u001b[0;36mcreate_save_markov_chains\u001b[0;34m()\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0mcolname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'('\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m')-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mfinal_transition_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m     \u001b[0mfinal_transition_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ClusterId'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m     \u001b[0mfinal_transition_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AvgLat'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAvgLat_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0mfinal_transition_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AvgLon'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAvgLon_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2518\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2519\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2521\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2584\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2585\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2586\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   2758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2759\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2760\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2761\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2762\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_sanitize_index\u001b[0;34m(data, index, copy)\u001b[0m\n\u001b[1;32m   3119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3120\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3121\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Length of values does not match length of '\u001b[0m \u001b[0;34m'index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3123\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPeriodIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values does not match length of index"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import math \n",
    "import os\n",
    "import errno\n",
    "import matplotlib.patches as patches\n",
    "from copy import deepcopy\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "from matplotlib.patches import Ellipse, Circle\n",
    "import operator\n",
    "import pdb\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "\n",
    "#-------------------------------\n",
    "def prepare_dfs():\n",
    "    global cluster_hourly_df    \n",
    "    \n",
    "    #create cluster_hourly_df columns\n",
    "    for i in range(0, 24):\n",
    "        cluster_hourly_df['Date'] = 0\n",
    "        cluster_hourly_df['ClusterId'] = 0\n",
    "        cluster_hourly_df['DistanceCluster'] = 0\n",
    "        cluster_hourly_df['AvgLat'] = 0\n",
    "        cluster_hourly_df['AvgLon'] = 0\n",
    "        cluster_hourly_df[i] = 0\n",
    "            \n",
    "#------------------------------------------------------------------------------------\n",
    "#Find distance between two lan:lon points in meters\n",
    "def meters(lat1, lon1, lat2, lon2):  \n",
    "    R = 6378.137 # Radius of earth in KM\n",
    "    dLat = lat2 * math.pi / 180 - lat1 * math.pi / 180\n",
    "    dLon = lon2 * math.pi / 180 - lon1 * math.pi / 180\n",
    "    a = math.sin(dLat/2) * math.sin(dLat/2) + math.cos(lat1 * math.pi / 180) * math.cos(lat2 * math.pi / 180) * math.sin(dLon/2) * math.sin(dLon/2);\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a));\n",
    "    d = R * c\n",
    "    return d * 1000 # meters\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "def cluster(newlat, newlon, row, count):\n",
    "    global curr_hr_df\n",
    "    \n",
    "    currcluster = curr_hr_df['ClusterId'][row-1]\n",
    "    curr_hr_df['ClusterId'][row] = -1\n",
    "    curr_hr_df['ClusterMeanLat'][row] = -1.0\n",
    "    curr_hr_df['ClusterMeanLon'][row] = -1.0\n",
    "    curr_hr_df['StayPoint'][row] = -1\n",
    "    curr_hr_df['DistanceCluster'][row] = -1\n",
    "    clulat = curr_hr_df['ClusterMeanLat'][row-1]\n",
    "    clulon = curr_hr_df['ClusterMeanLon'][row-1]\n",
    "    \n",
    "    if meters(clulat, clulon, newlat, newlon)<= 50:\n",
    "        curr_hr_df['ClusterId'][row] = currcluster\n",
    "        curr_hr_df['ClusterMeanLat'] = curr_hr_df.groupby('ClusterId')['Latitude'].transform(np.mean)\n",
    "        curr_hr_df['ClusterMeanLon'] = curr_hr_df.groupby('ClusterId')['Longitude'].transform(np.mean)\n",
    "        count = count + 1\n",
    "    else:\n",
    "        \n",
    "        if count >= 2:\n",
    "            MinClusTime = curr_hr_df['Timestamp'][row-count]\n",
    "            MaxClusTime = curr_hr_df['Timestamp'][row-1]\n",
    "            k = MaxClusTime - MinClusTime\n",
    "            l = (k / np.timedelta64(1, 'm')).astype(int)\n",
    "            \n",
    "            if (l >= 10):\n",
    "                curr_hr_df.loc[ (curr_hr_df['ClusterId']==currcluster), 'StayPoint'] = 1\n",
    "        count = 1\n",
    "        curr_hr_df['ClusterMeanLat'][row] = curr_hr_df['Latitude'][row]\n",
    "        curr_hr_df['ClusterMeanLon'][row] = curr_hr_df['Longitude'][row]\n",
    "        curr_hr_df['ClusterId'][row] = currcluster + 1\n",
    "    return count\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "def read_usr_file():\n",
    "    global usr_trejec_df\n",
    "    \n",
    "    #Load file names for user\n",
    "    filenames = glob.glob(file_source_raw)\n",
    "\n",
    "    #Read the files\n",
    "    list_of_dfs = [pd.read_csv(filename, skiprows=6, header = None) for filename in filenames]\n",
    "\n",
    "    #put the data from list into one dataframe\n",
    "    usr_trejec_df = pd.concat(list_of_dfs, ignore_index=True)\n",
    "    usr_trejec_df.columns = ['Latitude', 'Longitude', '0', 'Altitude', 'NumDays', 'Date', 'Time']\n",
    "    usr_trejec_df[\"Timestamp\"] = usr_trejec_df[\"Date\"].map(str) + \" \" + usr_trejec_df[\"Time\"]\n",
    "    \n",
    "    usr_trejec_df.index = pd.to_datetime(usr_trejec_df.Timestamp)\n",
    "    usr_trejec_df = usr_trejec_df.resample('1T').mean()\n",
    "    usr_trejec_df = usr_trejec_df.dropna()\n",
    "    \n",
    "     #add columns to user trajectory dataframe\n",
    "    #1. add timestamp as column\n",
    "    usr_trejec_df['Timestamp'] = pd.to_datetime(usr_trejec_df.index)\n",
    "    #restore date and time column\n",
    "    usr_trejec_df['Date'] = usr_trejec_df.Timestamp.dt.date\n",
    "    usr_trejec_df['Time'] = usr_trejec_df.Timestamp.dt.time\n",
    "    #2. add hour as column\n",
    "    usr_trejec_df['Hour'] = usr_trejec_df.Timestamp.dt.hour\n",
    "    #3. add weekday number.name as column\n",
    "    usr_trejec_df['Weekday'] = usr_trejec_df['Timestamp'].dt.weekday.map(str) + usr_trejec_df['Timestamp'].dt.weekday_name\n",
    "    #4. ClusterId, 5. ClusterMeanLat, 6. ClusterMeanLon, 7. StayPoint, 8. DistanceCluster\n",
    "    usr_trejec_df['ClusterId'] = -1\n",
    "    usr_trejec_df['ClusterMeanLat'] = -1.0\n",
    "    usr_trejec_df['ClusterMeanLon'] = -1.0\n",
    "    usr_trejec_df['StayPoint'] = -1\n",
    "    usr_trejec_df['DistanceCluster'] = -1\n",
    "    \n",
    "    #remove columns not used\n",
    "    usr_trejec_df = usr_trejec_df.drop(['0', 'Altitude', 'NumDays'], axis = 1)\n",
    "    \n",
    "    #sort the trajectory based on date and time\n",
    "    usr_trejec_df = usr_trejec_df.sort_values(['Date', 'Time'])\n",
    "    \n",
    "    #reset index\n",
    "    usr_trejec_df = usr_trejec_df.reset_index(drop=True)\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "def read_trained_model():\n",
    "    global trained_model_df\n",
    "    \n",
    "    trained_model_df = pd.read_csv(usr_markov_chains_file, header = 0, sep=\"\\t\")\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "def create_last_hr_staypts():\n",
    "    global curr_hr_df \n",
    "    global staypts_df\n",
    "    global curr_hr_staypts_df\n",
    "    \n",
    "    #clear current hour staypoints dataframe\n",
    "    curr_hr_staypts_df = curr_hr_staypts_df.iloc[0:0]\n",
    "    \n",
    "    #reset index\n",
    "    curr_hr_df = curr_hr_df.reset_index(drop=True)\n",
    "    \n",
    "    #Read the file in an online manner as the points come and assign the points to clusters\n",
    "    row =1\n",
    "    count = 1\n",
    "    if not staypts_df.empty:\n",
    "        curr_hr_df['ClusterId'][row-1] = staypts_df['ClusterId'].max() + 1\n",
    "    else:\n",
    "        curr_hr_df['ClusterId'][row-1] = 0\n",
    "    \n",
    "    curr_hr_df['ClusterMeanLat'][row-1] = curr_hr_df['Latitude'][0]\n",
    "    curr_hr_df['ClusterMeanLon'][row-1] = curr_hr_df['Longitude'][0]\n",
    "    curr_hr_df['StayPoint'][row-1] = -1\n",
    "    curr_hr_df['DistanceCluster'][row-1] = -1\n",
    "    \n",
    "    while row < len(curr_hr_df):\n",
    "        count = cluster(curr_hr_df['Latitude'][row], curr_hr_df['Longitude'][row], row, count)\n",
    "        row= row + 1\n",
    "    \n",
    "    #copy the staypoints to the current hour staypoints dataframe\n",
    "    curr_hr_staypts_df = curr_hr_df.loc[curr_hr_df['StayPoint'] == 1]\n",
    "    #copy the stay points into another dataframe\n",
    "    staypts_df = staypts_df.append(curr_hr_df.loc[curr_hr_df['StayPoint'] == 1])\n",
    "    #reset staypoints index\n",
    "    curr_hr_staypts_df.index = curr_hr_staypts_df['Timestamp']\n",
    "    staypts_df.index = staypts_df['Timestamp']\n",
    "    #clear current hour dataframe\n",
    "    curr_hr_df = curr_hr_df.iloc[0:0]\n",
    "    \n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "def cal_hourly_cluster_weight():\n",
    "    global curr_hr_staypts_df\n",
    "    global cluster_hourly_df   \n",
    "    \n",
    "    curr_hr_cluster_hourly_df = pd.DataFrame()       \n",
    "    \n",
    "    last_hour = curr_hr_staypts_df['Timestamp'][0].hour\n",
    "    last_clusid = curr_hr_staypts_df['ClusterId'][0]\n",
    "    curr_count = 0\n",
    "    j = 0\n",
    "    \n",
    "    for i in range(0, 24):\n",
    "        curr_hr_cluster_hourly_df['Date'] = 0\n",
    "        curr_hr_cluster_hourly_df['ClusterId'] = 0\n",
    "        curr_hr_cluster_hourly_df['DistanceCluster'] = 0\n",
    "        curr_hr_cluster_hourly_df['AvgLat'] = 0\n",
    "        curr_hr_cluster_hourly_df['AvgLon'] = 0\n",
    "        curr_hr_cluster_hourly_df[i] = 0\n",
    "        \n",
    "    for i in range(0, len(curr_hr_staypts_df)):\n",
    "\n",
    "        if (i == len(curr_hr_staypts_df)-1):\n",
    "            \n",
    "            k = curr_hr_staypts_df['Timestamp'][i] - curr_hr_staypts_df['Timestamp'][i-curr_count+1]\n",
    "            l = (k / np.timedelta64(1, 'm')).astype(int)\n",
    "            \n",
    "            date_read = curr_hr_staypts_df.index[i].date()\n",
    "            cluster_id = curr_hr_staypts_df['ClusterId'][i]\n",
    "            ClusterMeanLat = curr_hr_staypts_df['ClusterMeanLat'][i]\n",
    "            ClusterMeanLon = curr_hr_staypts_df['ClusterMeanLon'][i]\n",
    "            col_name = curr_hr_staypts_df.index[i].hour\n",
    "\n",
    "            curr_hr_cluster_hourly_df.loc[j,'AvgLat'] = ClusterMeanLat\n",
    "            curr_hr_cluster_hourly_df.loc[j,'AvgLon'] = ClusterMeanLon\n",
    "            curr_hr_cluster_hourly_df.loc[j,'Date'] = date_read\n",
    "            curr_hr_cluster_hourly_df.loc[j,'ClusterId'] = cluster_id\n",
    "            curr_hr_cluster_hourly_df.loc[j, col_name] = round((l)/60,4)\n",
    "            \n",
    "        if (curr_hr_staypts_df['Timestamp'][i].hour != last_hour) | (curr_hr_staypts_df['ClusterId'][i] != last_clusid):\n",
    "            #import pdb; pdb.set_trace()\n",
    "\n",
    "            if (curr_count == 1) & (curr_hr_staypts_df['Timestamp'][i].hour != last_hour):\n",
    "                k = ((curr_hr_staypts_df['Timestamp'][i-1] + pd.Timedelta(hours=1) - \n",
    "                      pd.Timedelta(minutes=curr_hr_staypts_df['Timestamp'][i-1].minute)) - \n",
    "                     curr_hr_staypts_df['Timestamp'][i-1])\n",
    "            else:\n",
    "                k = curr_hr_staypts_df['Timestamp'][i-1] - curr_hr_staypts_df['Timestamp'][i-curr_count]\n",
    "\n",
    "            l = (k / np.timedelta64(1, 'm')).astype(int)\n",
    "            date_read = curr_hr_staypts_df.index[i-1].date()\n",
    "            cluster_id = curr_hr_staypts_df['ClusterId'][i-1]\n",
    "            ClusterMeanLat = curr_hr_staypts_df['ClusterMeanLat'][i-1]\n",
    "            ClusterMeanLon = curr_hr_staypts_df['ClusterMeanLon'][i-1]\n",
    "            col_name = curr_hr_staypts_df.index[i-1].hour\n",
    "\n",
    "            curr_hr_cluster_hourly_df.loc[j, 'AvgLat'] = ClusterMeanLat\n",
    "            curr_hr_cluster_hourly_df.loc[j, 'AvgLon'] = ClusterMeanLon\n",
    "            curr_hr_cluster_hourly_df.loc[j, 'Date'] = date_read\n",
    "            curr_hr_cluster_hourly_df.loc[j, 'ClusterId'] = cluster_id\n",
    "            curr_hr_cluster_hourly_df.loc[j, col_name] = round((l)/60,4)\n",
    "            j = j + 1\n",
    "            curr_count = 1\n",
    "\n",
    "            if (curr_hr_staypts_df['Timestamp'][i].hour != last_hour):\n",
    "                last_hour = curr_hr_staypts_df['Timestamp'][i].hour\n",
    "            if (curr_hr_staypts_df['ClusterId'][i] != last_clusid):\n",
    "                last_clusid = curr_hr_staypts_df['ClusterId'][i]\n",
    "        else:\n",
    "            curr_count = curr_count + 1\n",
    "\n",
    "    curr_hr_cluster_hourly_df = curr_hr_cluster_hourly_df.fillna(0)\n",
    "    curr_hr_cluster_hourly_df = curr_hr_cluster_hourly_df.groupby(['Date', 'ClusterId','DistanceCluster', 'AvgLat', 'AvgLon']).sum()\n",
    "    curr_hr_cluster_hourly_df = curr_hr_cluster_hourly_df.reset_index(level=[0,1,2,3,4])\n",
    "   \n",
    "    cluster_hourly_df = cluster_hourly_df.append(curr_hr_cluster_hourly_df, ignore_index=True)\n",
    "    cluster_hourly_df = cluster_hourly_df.reset_index(drop=True)\n",
    "    \n",
    "#-------------group_clusters-----------------------------------------------------------------------\n",
    "def group_clusters():\n",
    "    global staypts_df\n",
    "    \n",
    "    #this fucntion groups the clusters together from different days \n",
    "    #Copy the stay points dataframe into another dataframe and remove duplicates\n",
    "    staypts_df1 = staypts_df[['ClusterId', 'ClusterMeanLat', 'ClusterMeanLon']].copy()\n",
    "    staypts_df1 = staypts_df1.drop_duplicates(subset=['ClusterId', 'ClusterMeanLat', 'ClusterMeanLon'])\n",
    "\n",
    "    staypts_df1 = staypts_df1.sort_values(['ClusterMeanLat', 'ClusterMeanLon'])\n",
    "\n",
    "    row = 1\n",
    "    for i in range(0, len(staypts_df1)):\n",
    "        for j in range(i+1, len(staypts_df1)):\n",
    "        \n",
    "            chk_cluster = staypts_df1['ClusterId'][i]\n",
    "            chk_clulat = staypts_df1['ClusterMeanLat'][i]\n",
    "            chk_clulon = staypts_df1['ClusterMeanLon'][i]\n",
    "            curr_cluster = staypts_df1['ClusterId'][j]\n",
    "            curr_clulat = staypts_df1['ClusterMeanLat'][j]\n",
    "            curr_clulon = staypts_df1['ClusterMeanLon'][j]\n",
    "        \n",
    "            if meters(chk_clulat, chk_clulon, curr_clulat, curr_clulon)<= 50:\n",
    "                staypts_df.loc[ (staypts_df['ClusterId']==curr_cluster), 'ClusterId'] = chk_cluster\n",
    "                staypts_df['ClusterMeanLat'] = staypts_df.groupby('ClusterId')['ClusterMeanLat'].transform(np.mean)\n",
    "                staypts_df['ClusterMeanLon'] = staypts_df.groupby('ClusterId')['ClusterMeanLon'].transform(np.mean)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "def update_staypts_csv():\n",
    "     with open(dest_file_staypoints, 'a') as f:\n",
    "             (staypts_df).to_csv(f,  sep='\\t', encoding='utf-8')\n",
    "#-----------------------------------------------------------------------------------\n",
    "def update_hourly_weights_csv():\n",
    "    with open(dest_file_hourly_weights, 'a') as f:\n",
    "             (cluster_hourly_df).to_csv(f,  sep='\\t', encoding='utf-8')\n",
    "#------------------------------------------------------------------------------------\n",
    "def create_save_seperate_trasition_matrices():\n",
    "    \n",
    "    #create a temp dataframe for pervious date\n",
    "    temp_df = pd.DataFrame()\n",
    "    matrices_df = pd.DataFrame()\n",
    "    temp_df = cluster_hourly_df.loc[cluster_hourly_df['Date'] == prev_date]\n",
    "    temp_df = temp_df.reset_index(drop=True)\n",
    "\n",
    "    for i in range(0, 24):\n",
    "        matrices_df['Date'] = 0\n",
    "        matrices_df['ClusterId'] = 0\n",
    "        for j in range(0, len(temp_df)):\n",
    "            colname = '(' + str(i) + '-' + str(i+1) + ')-' + str(temp_df['ClusterId'][j])\n",
    "            matrices_df[colname] = 0\n",
    "\n",
    "    matrices_df['Date'] = temp_df['Date']\n",
    "    matrices_df['ClusterId'] = temp_df['ClusterId']\n",
    "\n",
    "    for i in range (0, 23):\n",
    "        for j in range (0, len(temp_df)):\n",
    "            for k in range (0, len(temp_df)):\n",
    "                prob = temp_df[i][j] * temp_df[i+1][k]\n",
    "                colname = '(' + str(i) + '-' + str(i+1) + ')-' + str(temp_df['ClusterId'][k])\n",
    "                matrices_df[colname][j] = prob\n",
    "    file_name = dest_path_each_day_trsn_mat + str(prev_date) + \".csv\"\n",
    "    matrices_df.to_csv(file_name, sep='\\t', encoding='utf-8')\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "def create_save_markov_chains():\n",
    "    global final_transition_df\n",
    "    global co_loc\n",
    "    \n",
    "    #create an empty markov chain frame for each clusterid as state, and transition for each hour of the day\n",
    "    date_list = cluster_hourly_df['Date'].unique()\n",
    "    cluster_list = cluster_hourly_df['ClusterId'].unique()\n",
    "    AvgLat_list = cluster_hourly_df['AvgLat'].unique()\n",
    "    AvgLon_list = cluster_hourly_df['AvgLon'].unique()\n",
    "    \n",
    "    for i in range(0, 24):\n",
    "        final_transition_df['Address'] = 0\n",
    "        final_transition_df['AvgLat'] = 0\n",
    "        final_transition_df['AvgLon'] = 0\n",
    "        final_transition_df['ClusterId'] = 0\n",
    "        for j in range(0, cluster_hourly_df['ClusterId'].nunique()):\n",
    "            colname = '(' + str(i) + '-' + str(i+1) + ')-' + str(cluster_list[j])\n",
    "            final_transition_df[colname] = 0\n",
    "            \n",
    "    final_transition_df['ClusterId'] = cluster_list\n",
    "    final_transition_df['AvgLat'] = AvgLat_list\n",
    "    final_transition_df['AvgLon'] = AvgLon_list\n",
    "    final_transition_df = final_transition_df.fillna(0)\n",
    "    final_transition_df.index = final_transition_df.ClusterId\n",
    "\n",
    "    #read each day file and sum the matching rows:cols combinations\n",
    "    date_list = cluster_hourly_df['Date'].unique()\n",
    "    path_dir = dest_path_each_day_trsn_mat\n",
    "\n",
    "    \n",
    "    for p in range(0, cluster_hourly_df['Date'].nunique()):\n",
    "        temp_df = pd.DataFrame()\n",
    "        filename = path_dir + str(date_list[p]) + '.csv'\n",
    "        temp_df =  pd.read_csv(filename, header = 0, sep='\\t')\n",
    "\n",
    "        for i in range(0, len(temp_df)):\n",
    "            rowname = temp_df['ClusterId'][i]\n",
    "            for src_column in temp_df:\n",
    "                for dest_column in final_transition_df:\n",
    "                    if src_column == dest_column and src_column != 'ClusterId' :\n",
    "                        #import pdb; pdb.set_trace()\n",
    "                        final_transition_df[dest_column][rowname] = (final_transition_df[dest_column][rowname] +\n",
    "                                                                    temp_df[src_column][i])\n",
    "\n",
    "    #calculate probability from cluster x to cluster y from time t to t+1\n",
    "    final_transition_df = final_transition_df.reset_index(drop=True)\n",
    "    for clus in range(0, len(final_transition_df)):\n",
    "        for i in range(0, 24):\n",
    "            temp_sum = 0\n",
    "            for j in range(0, len(final_transition_df)):\n",
    "                colname = '(' + str(i) + '-' + str(i+1) + ')-' + str(final_transition_df['ClusterId'][j])\n",
    "                temp_sum += (final_transition_df[colname][clus])\n",
    "            for k in range(0, len(final_transition_df)):\n",
    "                colname = '(' + str(i) + '-' + str(i+1) + ')-' + str(final_transition_df['ClusterId'][k])\n",
    "                if temp_sum != 0:\n",
    "                    final_transition_df[colname][clus] = final_transition_df[colname][clus]/temp_sum\n",
    "    \n",
    "    #create dictionary for coordinate : address\n",
    "    points = tuple(zip(final_transition_df.AvgLat, final_transition_df.AvgLon))\n",
    "    geocoder = Nominatim(timeout=10)\n",
    "    coordinate_location = {}\n",
    "    for coordinate in points:\n",
    "        try:\n",
    "            location = geocoder.reverse(coordinate)\n",
    "        except:\n",
    "            location = 'unknown'\n",
    "        coordinate_location[coordinate] = location\n",
    "    co_loc = {k:v.raw for k,v in coordinate_location.items()}\n",
    "    \n",
    "    for i in range(0, len(final_transition_df)):\n",
    "        address = co_loc.get((final_transition_df['AvgLat'][i], final_transition_df['AvgLon'][i]))\n",
    "        final_transition_df['Address'][i] = address['display_name']\n",
    "    \n",
    "    #replace zero probabilities to a small value and save the file\n",
    "    final_transition_df = final_transition_df.fillna(0)                    \n",
    "    final_transition_df = final_transition_df.replace(0, 0.00001)\n",
    "    final_transition_df.to_csv(dest_file_final_markov_chain, sep='\\t')\n",
    "    \n",
    "    for i in range(0, 24):\n",
    "        final_transition_temp_df = pd.DataFrame()\n",
    "        k = cluster_hourly_df['ClusterId'].nunique()*i + 4\n",
    "        final_transition_temp_df = final_transition_df.iloc[:,k:k + cluster_hourly_df['ClusterId'].nunique()]\n",
    "        final_transition_temp_df.index = cluster_list\n",
    "        file_name = usr_markov_chains_directory + \"/\" + str(i) + \" hour.csv\"\n",
    "        final_transition_temp_df.to_csv(file_name, sep='\\t', encoding='utf-8')\n",
    "        \n",
    "        \n",
    "#------------------------------------------------------------------------------------\n",
    "def predict(new_lat, new_lon, new_hour, file_name):\n",
    "    global trained_model_df\n",
    "    global usr_trejec_df\n",
    "        \n",
    "    for i in range(0, len(trained_model_df)):\n",
    "        \n",
    "        trn_lat = trained_model_df['AvgLat'][i]\n",
    "        trn_lon = trained_model_df['AvgLon'][i]\n",
    "        if meters(trn_lat, trn_lon, new_lat, new_lon)<= 50:\n",
    "            \n",
    "            predic_df = pd.DataFrame()\n",
    "            \n",
    "            cluster_id = trained_model_df['ClusterId'][i]\n",
    "            curr_lat = trained_model_df['AvgLat'][i]\n",
    "            curr_lon = trained_model_df['AvgLon'][i]\n",
    "            curr_add = trained_model_df['Address'][i]\n",
    "            pred_loc = {\"current\":(cluster_id, curr_lat, curr_lon, curr_add)}\n",
    "            \n",
    "            from_col_no = trained_model_df['ClusterId'].nunique() * new_hour + 5\n",
    "            to_col_no = from_col_no + trained_model_df['ClusterId'].nunique()\n",
    "            predic_df = trained_model_df.iloc[i:i+1,from_col_no:to_col_no]\n",
    "            predic_df = predic_df.T\n",
    "            predic_df['ClusterId'] = cluster_id\n",
    "            predic_df['SelectedCluster'] = predic_df.index\n",
    "            predic_df['SelectedCluster'] = predic_df['SelectedCluster'].map(lambda x: x.split('-', 2)[-1])\n",
    "            predic_df.columns = ['Probability', 'ClusterId', 'SelectedCluster']\n",
    "            predic_df = predic_df.sort_values('Probability', ascending=False).head(10)\n",
    "            predic_df['Address'] = 0\n",
    "            predic_df['Latitude'] = 0.0\n",
    "            predic_df['Longitude'] = 0.0\n",
    "            predic_df = predic_df.reset_index(drop=True)\n",
    "            \n",
    "            for j in range (0, len(predic_df)):\n",
    "                #import pdb; pdb.set_trace()\n",
    "                clus_to_find = int(float(predic_df['SelectedCluster'][j]))\n",
    "                add = trained_model_df.loc[ (trained_model_df['ClusterId'] == clus_to_find), 'Address'].values[0]\n",
    "                lat = trained_model_df.loc[ (trained_model_df['ClusterId'] == clus_to_find), 'AvgLat'].values[0]\n",
    "                lon = trained_model_df.loc[ (trained_model_df['ClusterId'] == clus_to_find), 'AvgLon'].values[0]\n",
    "                \n",
    "                predic_df.loc[j, 'Address'] = add\n",
    "                predic_df.loc[j, 'Latitude'] = lat\n",
    "                predic_df.loc[j, 'Longitude'] = lon\n",
    "                \n",
    "            predic_df.to_csv(file_name, sep='\\t', encoding='utf-8')\n",
    "            break\n",
    "            \n",
    "#------------------------------------------ S T A R T -----------------------------------------------\n",
    "#global dataframes used\n",
    "#user raw trajectory dataframe\n",
    "usr_trejec_df = pd.DataFrame()\n",
    "#user trained model\n",
    "trained_model_df = pd.DataFrame()\n",
    "#current hour points\n",
    "curr_hr_df = pd.DataFrame()\n",
    "#current hour staypoints\n",
    "curr_hr_staypts_df = pd.DataFrame()\n",
    "#all staypoints\n",
    "staypts_df = pd.DataFrame()\n",
    "#hourly cluster\n",
    "cluster_hourly_df = pd.DataFrame()\n",
    "#final markov chains\n",
    "final_transition_df = pd.DataFrame()\n",
    "\n",
    "clus_dict = {}\n",
    "co_loc = {}\n",
    "global_clusterid = 0\n",
    "pred_loc = {}\n",
    "lat_array = []\n",
    "lon_array = []\n",
    "global_count = 0\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------CHANGE HERE FOR USER AND DATE RANGE--------------------------------------------\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#Edit user name, and path locations for source and destination files\n",
    "user = \"041\"\n",
    "\n",
    "\n",
    "#destination paths\n",
    "usr_directory = \"/home/shashank/Documents/location/code/test prediction quality/test 1/results/User \" + user\n",
    "usr_hrly_wght_directory = \"/home/shashank/Documents/location/code/test prediction quality/test 1/results/User \" + user + \"/hourlyweights\"\n",
    "usr_sty_pts_directory = \"/home/shashank/Documents/location/code/test prediction quality/test 1/results/User \" + user + \"/staypoints\"\n",
    "usr_markov_chains_directory = \"/home/shashank/Documents/location/code/test prediction quality/test 1/results/User \" + user + \"/markovchains\"\n",
    "\n",
    "if not os.path.exists(usr_directory):\n",
    "    os.makedirs(usr_directory)\n",
    "if not os.path.exists(usr_hrly_wght_directory):\n",
    "    os.makedirs(usr_hrly_wght_directory)\n",
    "if not os.path.exists(usr_sty_pts_directory):\n",
    "    os.makedirs(usr_sty_pts_directory)  \n",
    "if not os.path.exists(usr_markov_chains_directory):\n",
    "    os.makedirs(usr_markov_chains_directory)  \n",
    "\n",
    "dest_file_staypoints = usr_sty_pts_directory + \"/staypoints.csv\"\n",
    "dest_file_hourly_weights = usr_hrly_wght_directory + \"/hourlyweights.csv\"\n",
    "dest_path_each_day_trsn_mat = usr_hrly_wght_directory + \"/\"\n",
    "dest_file_final_markov_chain = usr_markov_chains_directory + \"/final.csv\"\n",
    "\n",
    "try:\n",
    "    os.remove(dest_file_staypoints)\n",
    "    os.remove(dest_file_hourly_weights)\n",
    "    os.remove(dest_file_final_markov_chain)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "#source paths\n",
    "file_source_raw = \"/home/shashank/Documents/location/Geolife Trajectories 1.3/Data/\" + user + \"/Trajectory/200906*.plt\" \n",
    "usr_markov_chains_file = \"/home/shashank/Documents/location/Data/User \" + user + \"/markovchains/final.csv\"\n",
    "dest_predicted_dir = \"/home/shashank/Documents/location/Data/User \" + user + \"/predict/\"\n",
    "dest_path_each_day_trsn_mat = usr_hrly_wght_directory + \"/\"\n",
    "dest_file_final_markov_chain = usr_markov_chains_directory + \"/final.csv\"\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------CHANGE HERE FOR USER AND DATE RANGE--------------------------------------------\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#read test user trajectory file. In real scenerio, this will be the GPS read data\n",
    "read_usr_file()\n",
    "\n",
    "#prepere dataframes\n",
    "prepare_dfs()\n",
    "\n",
    "#Save first date and time as prev date and time for the start\n",
    "prev_date = usr_trejec_df['Date'][0]\n",
    "prev_hour = usr_trejec_df['Hour'][0]\n",
    "\n",
    "#Read the new locations in an online fashion\n",
    "#  1. Everytime the hour changes, \n",
    "#                  A. Find staypoints for the last hour\n",
    "#                  B. Cluster staypoints based on distance for last hour\n",
    "#                  C. Calculate clusters hourly weights for last hour\n",
    "#                  D. Predict based on trained data\n",
    "#  2. Everytime the date changes,\n",
    "#                  A. Add the days data into training data\n",
    "for i in range(0, len(usr_trejec_df)):\n",
    "    \n",
    "    new_hour = usr_trejec_df['Hour'][i]\n",
    "    new_date = usr_trejec_df['Date'][i]\n",
    "    \n",
    "    if (new_date != prev_date):\n",
    "        update_staypts_csv()\n",
    "        update_hourly_weights_csv()\n",
    "        create_save_seperate_trasition_matrices()\n",
    "        create_save_markov_chains()\n",
    "        prev_date = new_date\n",
    "        \n",
    "    if (new_hour != prev_hour) and not curr_hr_df.empty:\n",
    "        \n",
    "        create_last_hr_staypts()\n",
    "        group_clusters()\n",
    "        \n",
    "        if not curr_hr_staypts_df.empty:\n",
    "            cal_hourly_cluster_weight()\n",
    "            #predict if training data is available\n",
    "            \n",
    "            #predict_last_hr()\n",
    "        prev_hour = new_hour \n",
    "        \n",
    "    else:\n",
    "        curr_hr_df = curr_hr_df.append(usr_trejec_df.iloc[[i]]) \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Address</th>\n",
       "      <th>AvgLat</th>\n",
       "      <th>AvgLon</th>\n",
       "      <th>ClusterId</th>\n",
       "      <th>(0-1)-1.0</th>\n",
       "      <th>(1-2)-1.0</th>\n",
       "      <th>(2-3)-1.0</th>\n",
       "      <th>(3-4)-1.0</th>\n",
       "      <th>(4-5)-1.0</th>\n",
       "      <th>(5-6)-1.0</th>\n",
       "      <th>...</th>\n",
       "      <th>(14-15)-23.0</th>\n",
       "      <th>(15-16)-23.0</th>\n",
       "      <th>(16-17)-23.0</th>\n",
       "      <th>(17-18)-23.0</th>\n",
       "      <th>(18-19)-23.0</th>\n",
       "      <th>(19-20)-23.0</th>\n",
       "      <th>(20-21)-23.0</th>\n",
       "      <th>(21-22)-23.0</th>\n",
       "      <th>(22-23)-23.0</th>\n",
       "      <th>(23-24)-23.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Address  AvgLat  AvgLon  ClusterId  (0-1)-1.0  (1-2)-1.0  (2-3)-1.0  \\\n",
       "0        0       0       0          0          0          0          0   \n",
       "\n",
       "   (3-4)-1.0  (4-5)-1.0  (5-6)-1.0      ...       (14-15)-23.0  (15-16)-23.0  \\\n",
       "0          0          0          0      ...                  0             0   \n",
       "\n",
       "   (16-17)-23.0  (17-18)-23.0  (18-19)-23.0  (19-20)-23.0  (20-21)-23.0  \\\n",
       "0             0             0             0             0             0   \n",
       "\n",
       "   (21-22)-23.0  (22-23)-23.0  (23-24)-23.0  \n",
       "0             0             0             0  \n",
       "\n",
       "[1 rows x 52 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#usr_trejec_df.loc[usr_trejec_df['Hour'] == 13]\n",
    "#curr_hr_df['ClusterId'][row-1] = staypts_df['ClusterId'].max() + 1\n",
    "#usr_trejec_df.loc[usr_trejec_df['Hour'] == 6]\n",
    "final_transition_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
